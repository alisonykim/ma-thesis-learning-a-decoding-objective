{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "g9cGIwNBEaMu"
      },
      "source": [
        "# Word attributes"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "f3udk3WeEaMx"
      },
      "source": [
        "### Definitions\n",
        "\n",
        "<strong>Shannon information</strong>: amount of information gained when an event occurs which had some probability value associated with it</br>\n",
        "* Mathematically: for some token $ x_i $ in a sequence $ X = \\langle x_1, x_2, ... \\rangle $ and its associated probability $ p(x_i) $, the information content of the token is given by $$ h(x_i) = -\\log_{2}{p(x_i)} \\text{ bits} $$<br>\n",
        "* An token with $ p = 1 $ would yield an information content of $ 0 \\text{ bits} $ ‚Äî¬†no new information is gained.<br>\n",
        "\n",
        "<strong>Shannon entropy</strong>: average number of bits required to represent or transmit a message without losing any data<br>\n",
        "* Mathematically: the entropy of a distribution $ P $ is given by the expected information content $$ H(X) = -\\sum\\limits_{x \\in \\mathcal{X}} {P(x) \\log_2{P(x)}} \\text{ bits} $$ where $ X $ is a discrete random variable that takes values in the alphabet $ \\mathcal{X} $ and is distributed according to $ p : \\mathcal{X} \\rightarrow [0, 1] $\n",
        "\t* Note: In machine learning, $ \\ln $ used rather than $ \\log_{2} $ as in information theory\n",
        "* <strong>Entropy is a function of a single distribution</strong> $ P $\n",
        "* Equivalent to <strong>average information content</strong>\n",
        "\n",
        "\n",
        "<strong>Cross-entropy:</strong> expected entropy under the <em>true</em> distribution $ P $ but drawn from <em>estimated</em> distribution $ Q $\n",
        "* Plain English: Imagine we're sending encoded messages where the underlying data is drawn from a data-generating distribution $ P $ (true distribution), but while using an encoding scheme optimized for an estimated distribution $ Q $. The cross-entropy is the expected length of a message encoded according to  ùëÑ  but drawn according to  ùëÉ .\n",
        "* Mathematically: $$ H(P, Q) = -\\sum\\limits_{x \\in \\mathcal{X}} P(x) \\log_{2}{Q(x)} $$\n",
        "* Note: $ H(L) \\leq H(L, M) $, i.e. the cross-entropy is bounded by the true entropy of the language \n",
        "* Expected message length according to $ Q $ but drawn from $ P $\n",
        "* <strong>Cross-entropy is thus a function of both $ P $ and $ Q $</strong>\n",
        "\n",
        "<strong>Perplexity</strong>: measures degree of uncertainty of a model in predicting (i.e. assigning probabilities to) text\n",
        "* Mathematically: exponentiated cross-entropy between the data and model predictions $$ p = \\exp{-\\frac{1}{t} \\sum\\limits_{i}^{t} \\log{p_{\\theta}(x_i | x_{< i})}} $$\n",
        "where $ \\log{p_{\\theta}(x_i | x_{< i})} $ is the log-likelihood of the $i$-th token conditioned on the preceding tokens $ x_{< i} $ according to our model\n",
        "* Model's ability to predict uniformly among the set of specified tokens in a corpus\n",
        "* Note: the tokenization procedure has a direct impact on a model's perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcURlDx6EaMy"
      },
      "outputs": [],
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = 'all'\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUaCB0NiEdUs"
      },
      "outputs": [],
      "source": [
        "# Ensure that necessary external libraries are installed\n",
        "!pip install datasets\n",
        "!pip install matplotlib\n",
        "!pip install numpy\n",
        "!pip install seaborn\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install tqdm"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7c_GW-A-EaMz"
      },
      "source": [
        "### <strong>Language modeling</strong> with GPT2 and WikiText 103"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzP4etx7EaMz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
        "\n",
        "# Load model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using device: { device.upper() }')\n",
        "model_checkpoint = 'gpt2'\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\n",
        "\tmodel_checkpoint,\n",
        "\tadd_special_tokens=True\n",
        ")\n",
        "model = GPT2LMHeadModel.from_pretrained(\n",
        "\tmodel_checkpoint\n",
        ")\n",
        "\n",
        "# Load dataset\n",
        "test = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')['text'] # Wikitext\n",
        "print(f'Number of texts: { len(test) }')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YN26JmY5EaM0"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "max_input_length = model.config.n_positions # GPT2\n",
        "print(f'GPT2 model maximum input length: { max_input_length }')\n",
        "\n",
        "# Model will have minimum <stride> context tokens when calculating conditional likelihood of a token\n",
        "# (Provided there are <stride> preceding tokens available to condition on)\n",
        "# These tokens are set to -100 so that they're ignored for the loss calculation\n",
        "stride = 512\n",
        "\n",
        "all_out = list()\n",
        "num_examples = 4\n",
        "\n",
        "for text in tqdm(test[:num_examples]):\n",
        "\ttmp = dict()\n",
        "\t\n",
        "\tif len(text.split()) < 10: # Ignore short texts\n",
        "\t\tcontinue\n",
        "\n",
        "\t# Convert prompt to model input IDS\n",
        "\tencodings = tokenizer(\n",
        "\t\ttext=text,\n",
        "\t\tmax_length=max_input_length-2, # subtract 2 because we want to add BOS and EOS tokens\n",
        "\t\ttruncation=True,\n",
        "\t\treturn_offsets_mapping=True\n",
        "\t)\n",
        "\tinput_ids = torch.tensor(\n",
        "\t\tdata=[tokenizer.bos_token_id]+encodings['input_ids']+[tokenizer.eos_token_id],\n",
        "\t\tdevice=model.device\n",
        "\t)\n",
        "\n",
        "\twith torch.no_grad(): # Suppress gradient calculation to speed up computation\n",
        "\t\toutput = model(\n",
        "\t\t\tinput_ids=input_ids,\n",
        "\t\t\tlabels=input_ids\n",
        "\t\t)\n",
        "\t\n",
        "\t# logit(p) = log(p/(1-p)) transforms probability values => domain [0, 1] range [-inf, +inf]\n",
        "\t# shift logits: up to index -1 because we don't care about probability distribution over tokens after EOS\n",
        "\tlogits_shifted = output['logits'][..., :-1, :].contiguous() # input ids x vocabulary\n",
        "\n",
        "\t# labels = ground truths taken from <tensor_input>\n",
        "\t# originally, logits[0] is the distribution conditioned on labels[0], but for logits we actually care about the probability conditioned on the preceding token\n",
        "\t# (as consistent with language modeling: what is the probability of token x_t given x_<t?)\n",
        "\t# => shift labels: align position 1 of the labels with position 0 of the logits => logits[0] should correspond to labels[1]\n",
        "\tlabels_shifted = input_ids[..., 1:].contiguous() # input ids x 1\n",
        "\n",
        "\t# Here, multi-class cross entropy is equivalent to NLL calculation and is calculated in log-e\n",
        "\t# CE loss: H(P, Q) = -\\sum(x in X) P(x) \\log_{2}{Q(x)}, where Q(x) is the estimated distribution\n",
        "\t# Note: this calculation is equivalent to calculating probs * -1og_probs, then for each token, select column corresponding to vocab index (see sanity check below)\n",
        "\tce_loss = torch.nn.functional.cross_entropy( # input ids x 1\n",
        "\t\tinput=logits_shifted.view(-1, logits_shifted.size(-1)), # Predicted unnormalized logits\n",
        "\t\ttarget=labels_shifted.view(-1), # Ground truth class labels (one-hot encoded)\n",
        "\t\treduction='none' # No reduction applied to cross entropy output\n",
        "\t)\n",
        "\t\n",
        "\t# Sanity check: mean CE loss should be close to automatic loss calculation\n",
        "\tloss = output['loss']\n",
        "\t# print(f'\\nMean cross entropy: { mean_ce_loss }')\n",
        "\t# print(f'Automatic model loss calculation: { loss }')\n",
        "\tassert torch.isclose(\n",
        "\t\tinput=torch.mean(ce_loss), # Loss defined as mean NLL\n",
        "\t\tother=loss\n",
        "\t)\n",
        "\t\n",
        "\t# Softmax to normalize logits\n",
        "\tprobs = logits_shifted.softmax(dim=-1) # input ids x vocabulary\n",
        "\ttoken_probs = torch.tensor([prob[vocab_idx] for vocab_idx, prob in zip(labels_shifted, probs)]) # input ids x 1\n",
        "\t\n",
        "\t# Sanity check: probabilities across each encoding (row) sums to 1\n",
        "\tfor idx, vocab_token in enumerate(probs):\n",
        "\t\tassert torch.isclose(\n",
        "\t\t\tinput=torch.sum(vocab_token),\n",
        "\t\t\tother=torch.tensor(1.),\n",
        "\t\t\trtol=0.01\n",
        "\t\t)\n",
        "\t# Sanity check: equivalently, sum of all elements in probabilities tensor should equal number of tokens\n",
        "\tsum_probs = torch.sum(probs, dtype=torch.float32)\n",
        "\tlen_labels = torch.tensor([labels_shifted.size()[0]], dtype=torch.float32)\n",
        "\t# print(f'Sum of all elements in probabilities tensor: { sum_probs }')\n",
        "\t# print(f'Number of tokens: { len_labels }')\n",
        "\tassert torch.isclose(\n",
        "\t\tinput=sum_probs,\n",
        "\t\tother=len_labels,\n",
        "\t\trtol=0.01\n",
        "\t)\n",
        "\n",
        "\t# Sanity check: NLL of token at token index equals CE loss\n",
        "\tnlls = -1 * logits_shifted.log_softmax(dim=-1)\n",
        "\tics = torch.tensor([nll[vocab_idx] for vocab_idx, nll in zip(labels_shifted, nlls)])\n",
        "\tassert torch.allclose(\n",
        "\t\tinput=ics,\n",
        "\t\tother=ce_loss\n",
        "\t)\n",
        "\n",
        "\t# \n",
        "\tper_token_entropy = probs * nlls # input ids x vocabulary\n",
        "\tper_token_entropy = torch.sum(per_token_entropy, dim=1) # input ids x 1, dim=1 means sum along each row\n",
        "\n",
        "\t# Sanity check: number of entropies calculated equals number of tokens minus 1 (shifted logits and labels)\n",
        "\t# print(f'Expected: { torch.tensor(input_ids.size()) - torch.tensor([1]) }')\n",
        "\t# print(f'Actual: { per_token_entropy.size() }\\n')\n",
        "\tassert torch.tensor(per_token_entropy.size()) == torch.tensor(input_ids.size()) - torch.tensor([1])\n",
        "\n",
        "\ttmp['text'] = text\n",
        "\ttmp['total_tokens'] = labels_shifted.size()[0]\n",
        "\ttmp['tensor_input'] = input_ids\n",
        "\ttmp['logits'] = logits_shifted\n",
        "\ttmp['labels'] = labels_shifted\n",
        "\ttmp['probs'] = token_probs # probs but at the correct indices => input ids x 1\n",
        "\ttmp['nlls'] = nlls # input ids x vocabulary\n",
        "\ttmp['ics'] = ics # nlls but at the correct indices (equivalently ce_loss and per token NLL) => input ids x 1\n",
        "\ttmp['per_token_entropy'] = per_token_entropy\n",
        "\n",
        "\tall_out.append(tmp)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-aR_UNI5EaM0"
      },
      "source": [
        "### Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qt-7mws9EaM1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "sns.set() # Set sns as default style"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDlLA7f3EaM2",
        "outputId": "3827fedf-6a4e-404b-88ec-fd2e2f572ea8"
      },
      "outputs": [],
      "source": [
        "print(f'Number of texts processed: { len(all_out) }')\n",
        "\n",
        "all_token_probs = torch.cat([text['probs'] for text in all_out])\n",
        "all_ics = torch.cat([text['ics'] for text in all_out])\n",
        "all_entropies = torch.cat([text['per_token_entropy'] for text in all_out])\n",
        "assert all_ics.size() == all_entropies.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPq40YafLPtS",
        "outputId": "26946af5-2984-4811-f52f-3824ea98dad4"
      },
      "outputs": [],
      "source": [
        "# Perplexity\n",
        "mean_nll = torch.mean(all_ics)\n",
        "perplexity = torch.exp(mean_nll)\n",
        "print(f'Perplexity: { perplexity }')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "JYc2sxwVEaM2",
        "outputId": "059c4940-9b8b-4e1b-c467-5d35309968fe"
      },
      "outputs": [],
      "source": [
        "#### NLL & ENTROPY\n",
        "## Density plot\n",
        "fig1, (ics, entropies) = plt.subplots(1, 2, sharex=True, sharey=True);\n",
        "fig1.suptitle('Token attributes');\n",
        "\n",
        "# Information content (NLL)\n",
        "sns.kdeplot(\n",
        "\tax=ics,\n",
        "\tdata=all_ics,\n",
        "\tbw_method=0.5,\n",
        "\tcolor='green',\n",
        "\tfill=True,\n",
        "\tlabel='IC'\n",
        ");\n",
        "ics.set(title='Negative log likelihood');\n",
        "ics.set(xlabel='Bits');\n",
        "ics.set(xlim=(0));\n",
        "\n",
        "# Entropy (expected IC)\n",
        "sns.kdeplot(\n",
        "\tax=entropies,\n",
        "\tdata=all_entropies,\n",
        "\tbw_method=0.5,\n",
        "\tcolor='blue',\n",
        "\tfill=True,\n",
        "\tlabel='Entropy'\n",
        ");\n",
        "entropies.set(title='Entropy');\n",
        "entropies.set(xlabel='Bits');\n",
        "entropies.set(xlim=(0));\n",
        "entropies.xaxis.set_major_locator(ticker.MultipleLocator(2));\n",
        "entropies.xaxis.set_major_formatter(ticker.ScalarFormatter());\n",
        "\n",
        "plt.show();\n",
        "\n",
        "\n",
        "#### DIFF(NLL, ENTROPY)\n",
        "diff_ic_ent = torch.sub(all_ics, all_entropies);\n",
        "# density_diff_ic_ent = sns.kdeplot(\n",
        "# \tdata=diff_ic_ent,\n",
        "# \tbw_method=1,\n",
        "# \tcolor='red',\n",
        "# \tfill=True,\n",
        "# \tlabel='diff'\n",
        "# \t# clip=(0, 100)\n",
        "# )\n",
        "# density_diff_ic_ent.set(\n",
        "# \ttitle='diff(IC, entropy)',\n",
        "# \txlabel='Bits',\n",
        "# \tylabel='Density'\n",
        "# )\n",
        "\n",
        "# abs(diff)\n",
        "abs_diff_ic_ent = torch.abs(diff_ic_ent);\n",
        "\n",
        "density_abs_diff_ic_ent = sns.kdeplot(\n",
        "\tdata=abs_diff_ic_ent,\n",
        "\tbw_method=1,\n",
        "\tcolor='orange',\n",
        "\tfill=True,\n",
        "\tlabel='abs. diff'\n",
        "\t# clip=(0, 100)\n",
        ");\n",
        "\n",
        "# sq(diff)\n",
        "sq_diff_ic_ent = torch.square(diff_ic_ent)\n",
        "density_sq_diff_ic_ent = sns.kdeplot(\n",
        "\tdata=sq_diff_ic_ent,\n",
        "\tbw_method=1,\n",
        "\tcolor='yellow',\n",
        "\tfill=True,\n",
        "\tlabel='sq. diff'\n",
        "\t# clip=(0, 100)\n",
        ");\n",
        "\n",
        "xmax = max(sq_diff_ic_ent)+1;\n",
        "density_sq_diff_ic_ent.set_title('Deviation of IC from expected IC');\n",
        "density_sq_diff_ic_ent.legend(loc=0);\n",
        "density_sq_diff_ic_ent.set(xlim=(0, xmax));\n",
        "density_sq_diff_ic_ent.set(xlabel='Bits');\n",
        "\n",
        "plt.show();\n",
        "\n",
        "print(f'Mean abs diff: { torch.mean(abs_diff_ic_ent) }');\n",
        "print(f'Median abs diff: { torch.median(abs_diff_ic_ent) }');\n",
        "print(f'Max abs diff: { torch.max(abs_diff_ic_ent) }');\n",
        "print(f'Min abs diff: { torch.min(abs_diff_ic_ent) }');"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6YuGoaBKEaM3"
      },
      "source": [
        "### <strong>Abstractive summarization</strong> with T5 and CNN DailyMail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHtPtb2SEaM3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
        "\n",
        "\n",
        "# Load model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using device: { device.upper() }')\n",
        "model_checkpoint = 't5-small'\n",
        "tokenizer = T5TokenizerFast.from_pretrained(model_checkpoint)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Load dataset\n",
        "test_articles = load_dataset('cnn_dailymail', '1.0.0', split='test')['article'] # article\n",
        "test_summaries = load_dataset('cnn_dailymail', '1.0.0', split='test')['highlights'] # summaries\n",
        "print(f'Number of texts: { len(test_articles) }')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWAhMzzXEaM3",
        "outputId": "cc39e852-2248-4619-a558-04dc7b8c3a79"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "len_articles = [len(tokenizer.encode(article)) for article in test_articles]\n",
        "len_summaries = [len(tokenizer.encode(summary)) for summary in test_summaries]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 3.5), sharey=True);\n",
        "axes[0].hist(len_articles, bins=20, color=\"C0\", edgecolor=\"C0\");\n",
        "axes[0].set_title('Article Token Length');\n",
        "axes[0].set_xlabel('Length (Tokens)');\n",
        "axes[0].set_ylabel('Count');\n",
        "axes[1].hist(len_summaries, bins=20, color=\"C0\", edgecolor=\"C0\");\n",
        "axes[1].set_title('Summary Token Length');\n",
        "axes[1].set_xlabel('Length (Tokens)');\n",
        "plt.tight_layout();\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8mkztBnEaM3"
      },
      "outputs": [],
      "source": [
        "import traceback\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Model will have minimum <stride> context tokens when calculating conditional likelihood of a token\n",
        "# (Provided there are <stride> preceding tokens available to condition on)\n",
        "# These tokens are set to -100 so that they're ignored for the loss calculation\n",
        "stride = 512\n",
        "\n",
        "all_out = list()\n",
        "num_examples = 10\n",
        "\n",
        "for article, ref_summary in zip(tqdm(test_articles[:num_examples]), tqdm(test_summaries[:num_examples])):\n",
        "\t# Initialize hash, counter\n",
        "\ttmp = dict()\n",
        "\n",
        "\t# Prepare article for summarization per T5 config\n",
        "\tarticle = 'summarize: ' + article\n",
        "\n",
        "\t# Convert source text to model input IDs\n",
        "\tencodings_source = tokenizer(\n",
        "\t\ttext=article,\n",
        "\t\ttruncation=True,\n",
        "\t\treturn_offsets_mapping=True\n",
        "\t)\n",
        "\tinput_ids = torch.tensor( # input ids (including EOS token) x 1\n",
        "\t\tdata=encodings_source['input_ids']+[tokenizer.eos_token_id],\n",
        "\t\tdevice=model.device\n",
        "\t)\n",
        "\tinput_ids = input_ids.unsqueeze(dim=0) # T5 model needs tensor([[]]), encodings alone return tensor([])\n",
        "\tprint(f'Input IDs (encodings + EOS): { input_ids.size() }')\n",
        "\n",
        "\t# Convert target text to model decoder input IDs and labels\n",
        "\tencodings_target = tokenizer(\n",
        "\t\ttext=ref_summary,\n",
        "\t\ttruncation=True,\n",
        "\t\treturn_offsets_mapping=True,\n",
        "\t)\n",
        "\tlabels = torch.tensor( # input ids (including EOS token) x 1\n",
        "\t\tdata=encodings_target['input_ids']+[tokenizer.eos_token_id],\n",
        "\t\tdevice=model.device\n",
        "\t)\n",
        "\tlabels = labels.unsqueeze(dim=0) # T5 model needs tensor([[]]), encodings alone return tensor([])\n",
        "\tprint(f'Labels: { labels.size() }')\n",
        "\n",
        "\twith torch.no_grad():\n",
        "\t\toutput = model( # not using model.generate since it doesn't support loss calculation\n",
        "\t\t\tinput_ids=input_ids,\n",
        "\t\t\tlabels=labels\n",
        "\t\t)\n",
        "\t\n",
        "\t# logit(p) = log(p/(1-p)) transforms probability values => domain [0, 1] range [-inf, +inf]\n",
        "\t# unlike LM, don't need to shift logits for AS\n",
        "\tlogits = output['logits'].squeeze()\n",
        "\n",
        "\t# labels = ground truths taken from reference summary\n",
        "\t# thus, don't need to shift labels\n",
        "\tlabels = labels.contiguous().squeeze(dim=0)\n",
        "\n",
        "\t# NLL = multi-class cross entropy (in this case) --> calculated in log-e\n",
        "\t# CE loss: H(P, Q) = -\\sum(x in X) P(x) \\log_{2}{Q(x)}, where Q(x) is the estimated distribution\n",
        "\t# Note: this calculation is equivalent to calculating probs * -log_probs, then for each token, select column corresponding to vocab index (see sanity check below)\n",
        "\t# Equivalent to IC: higher P(w) --> lower IC (relatively little info gained)\n",
        "\tce_loss = torch.nn.functional.cross_entropy( # input ids x 1\n",
        "\t\tinput=logits.view(-1, logits.size(-1)), # Predicted unnormalized logits\n",
        "\t\ttarget=labels.view(-1), # Ground truth class labels (one-hot encoded)\n",
        "\t\treduction='none' # No reduction applied to cross entropy output\n",
        "\t)\n",
        "\n",
        "\t# Sanity check: mean NLL should be close to automatic loss calculation\n",
        "\t# print(f'Mean NLL: { torch.mean(ce_loss) }')\n",
        "\t# print(f'Automatic loss calculation: { output[\"loss\"] }')\n",
        "\tassert torch.isclose(\n",
        "\t\tinput=torch.mean(ce_loss),\n",
        "\t\tother=output['loss'],\n",
        "\t\trtol=0.1\n",
        "\t)\n",
        "\n",
        "\t# Softmax to normalize logits\n",
        "\tprobs = logits.softmax(dim=-1) # input ids x vocabulary\n",
        "\tprint(f'Probs { probs.size() }:\\n{ probs }')\n",
        "\ttoken_probs = torch.tensor([prob[vocab_idx] for vocab_idx, prob in zip(labels, probs)]) # input ids x 1\n",
        "\tprint(f'Token probs { token_probs.size() }:\\n{ token_probs }')\n",
        "\n",
        "\t# Sanity check: probabilities across each encoding (row) sums to 1\n",
        "\tfor idx, vocab_token in enumerate(probs):\n",
        "\t\tassert torch.isclose(\n",
        "\t\t\tinput=torch.sum(vocab_token),\n",
        "\t\t\tother=torch.tensor(1.),\n",
        "\t\t\trtol=0.01\n",
        "\t\t)\n",
        "\t# Sanity check: equivalently, sum of all elements in probabilities tensor should equal number of tokens\n",
        "\tsum_probs = torch.sum(probs, dtype=torch.float32)\n",
        "\tlen_labels = torch.tensor([labels.size()[0]], dtype=torch.float32)\n",
        "\t# print(f'Sum of all elements in probabilities tensor: { sum_probs }')\n",
        "\t# print(f'Number of tokens: { len_labels }')\n",
        "\tassert torch.isclose(\n",
        "\t\tinput=sum_probs,\n",
        "\t\tother=len_labels,\n",
        "\t\trtol=0.01\n",
        "\t)\n",
        "\n",
        "\t# Sanity check: NLL of token at token index equals CE loss\n",
        "\tnlls = -1 * logits.log_softmax(dim=-1) # input ids x vocabularly\n",
        "\tics = torch.tensor([nll[vocab_idx] for vocab_idx, nll in zip(labels, nlls)]) # input ids x 1\n",
        "\tassert torch.allclose(\n",
        "\t\tinput=ics,\n",
        "\t\tother=ce_loss\n",
        "\t)\n",
        "\t\n",
        "\t# Entropy (a.k.a. expected IC): sum_{v in vocab} p(v) log p(v) at each token position (sum along row)\n",
        "\tper_token_entropy = probs * nlls # input ids x vocabulary\n",
        "\tper_token_entropy = torch.sum(per_token_entropy, dim=1) # input ids x 1, dim=1 means sum along each row (token)\n",
        "\n",
        "\t# Sanity check: number of entropies calculated equals number of summary tokens\n",
        "\tprint(f'Expected: { torch.tensor(labels.size()) }')\n",
        "\tprint(f'Actual: { torch.tensor(per_token_entropy.size()) }\\n')\n",
        "\tassert torch.tensor(per_token_entropy.size()) == torch.tensor(labels.size())\n",
        "\t\n",
        "\ttmp['article'] = article\n",
        "\ttmp['ref_summary'] = ref_summary\n",
        "\ttmp['tensor_input'] = input_ids\n",
        "\ttmp['logits'] = logits\n",
        "\ttmp['labels'] = labels\n",
        "\ttmp['probs'] = token_probs # probs but at the correct indices => input ids x 1\n",
        "\ttmp['nlls'] = nlls # input ids x vocabulary\n",
        "\ttmp['ics'] = ics # nlls but at the correct indices (equivalently ce_loss and per token NLL) => input ids x 1\n",
        "\ttmp['per_token_entropy'] = per_token_entropy\n",
        "\n",
        "\tall_out.append(tmp)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4eT24usadLqc"
      },
      "source": [
        "### Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0L4blWWBf1jQ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFfAGiQLf72r",
        "outputId": "08ade9b7-1980-4631-a565-2d09326003c4"
      },
      "outputs": [],
      "source": [
        "print(f'Number of articles summarized: { len(all_out) }')\n",
        "\n",
        "all_token_probs = torch.cat([text['probs'] for text in all_out])\n",
        "all_ics = torch.cat([text['ics'] for text in all_out])\n",
        "all_entropies = torch.cat([text['per_token_entropy'] for text in all_out])\n",
        "assert all_ics.size() == all_entropies.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JN6G6Uzdf-nP",
        "outputId": "480d77ac-0463-4b10-d371-4d161071fac6"
      },
      "outputs": [],
      "source": [
        "# Perplexity\n",
        "mean_nll = torch.mean(all_ics)\n",
        "perplexity = torch.exp(mean_nll)\n",
        "print(f'Perplexity: { perplexity }')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "ZTw5vPIFgAlB",
        "outputId": "6bf194aa-1a0c-4a0c-8bf3-3b1755e087c7"
      },
      "outputs": [],
      "source": [
        "#### NLL & ENTROPY\n",
        "## Density plot\n",
        "fig1, (ics, entropies) = plt.subplots(1, 2, sharex=False, sharey=False);\n",
        "fig1.suptitle('Token attributes');\n",
        "\n",
        "# Information content (NLL)\n",
        "sns.kdeplot(\n",
        "\tax=ics,\n",
        "\tdata=all_ics,\n",
        "\tbw_method=0.5,\n",
        "\tcolor='green',\n",
        "\tfill=True,\n",
        "\tlabel='IC'\n",
        ");\n",
        "ics.set(title='Negative log likelihood');\n",
        "ics.set(xlabel='Bits');\n",
        "\n",
        "# Entropy (expected IC)\n",
        "sns.kdeplot(\n",
        "\tax=entropies,\n",
        "\tdata=all_entropies,\n",
        "\tbw_method=0.5,\n",
        "\tcolor='blue',\n",
        "\tfill=True,\n",
        "\tlabel='Entropy'\n",
        ");\n",
        "entropies.set(title='Entropy');\n",
        "entropies.set(xlabel='Bits');\n",
        "# entropies.xaxis.set_major_locator(ticker.MultipleLocator(2));\n",
        "# entropies.xaxis.set_major_formatter(ticker.ScalarFormatter());\n",
        "\n",
        "plt.show();\n",
        "\n",
        "# abs(diff)\n",
        "diff_ic_ent = torch.sub(all_ics, all_entropies);\n",
        "\n",
        "abs_diff_ic_ent = torch.abs(diff_ic_ent);\n",
        "\n",
        "density_abs_diff_ic_ent = sns.kdeplot(\n",
        "\tdata=abs_diff_ic_ent,\n",
        "\tbw_method=1,\n",
        "\tcolor='orange',\n",
        "\tfill=True,\n",
        "\tlabel='abs. diff'\n",
        "\t# clip=(0, 100)\n",
        ");\n",
        "\n",
        "# sq(diff)\n",
        "sq_diff_ic_ent = torch.square(diff_ic_ent)\n",
        "density_sq_diff_ic_ent = sns.kdeplot(\n",
        "\tdata=sq_diff_ic_ent,\n",
        "\tbw_method=1,\n",
        "\tcolor='yellow',\n",
        "\tfill=True,\n",
        "\tlabel='sq. diff'\n",
        "\t# clip=(0, 100)\n",
        ");\n",
        "\n",
        "xmax = max(sq_diff_ic_ent);\n",
        "density_sq_diff_ic_ent.set_title('Deviation of IC from expected IC');\n",
        "density_sq_diff_ic_ent.legend(loc=0);\n",
        "density_sq_diff_ic_ent.set(xlim=(0, xmax));\n",
        "density_sq_diff_ic_ent.set(xlabel='Bits');\n",
        "\n",
        "plt.show();\n",
        "\n",
        "print(f'Mean abs diff: { torch.mean(abs_diff_ic_ent) }');\n",
        "print(f'Median abs diff: { torch.median(abs_diff_ic_ent) }');\n",
        "print(f'Max abs diff: { torch.max(abs_diff_ic_ent) }');\n",
        "print(f'Min abs diff: { torch.min(abs_diff_ic_ent) }');"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aITyqsV2bcyU"
      },
      "source": [
        "### <strong>Abstractive summarization</strong> with BART and CNN DailyMail\n",
        "* Note: BART tokenizer automatically adds BOS and EOS tokens, so there is no need to add them manually to the model inputs/labels (as done in the LM implementation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSer3OAAbcyV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import BartTokenizerFast, BartForConditionalGeneration, BartConfig\n",
        "\n",
        "\n",
        "# Load model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using device: { device.upper() }')\n",
        "model_checkpoint = 'facebook/bart-large-cnn'\n",
        "tokenizer = BartTokenizerFast.from_pretrained(model_checkpoint)\n",
        "if tokenizer.pad_token is None:\n",
        "\ttokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "model = BartForConditionalGeneration.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Load dataset\n",
        "test_articles = load_dataset('cnn_dailymail', '1.0.0', split='test')['article'] # article\n",
        "test_summaries = load_dataset('cnn_dailymail', '1.0.0', split='test')['highlights'] # summaries\n",
        "print(f'Number of texts: { len(test_articles) }')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKL1HRN8bcyV",
        "outputId": "cc39e852-2248-4619-a558-04dc7b8c3a79"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "len_articles = [len(tokenizer.encode(article)) for article in test_articles]\n",
        "len_summaries = [len(tokenizer.encode(summary)) for summary in test_summaries]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 3.5), sharey=True);\n",
        "axes[0].hist(len_articles, bins=20, color=\"C0\", edgecolor=\"C0\");\n",
        "axes[0].set_title('Article Token Length');\n",
        "axes[0].set_xlabel('Length (Tokens)');\n",
        "axes[0].set_ylabel('Count');\n",
        "axes[1].hist(len_summaries, bins=20, color=\"C0\", edgecolor=\"C0\");\n",
        "axes[1].set_title('Summary Token Length');\n",
        "axes[1].set_xlabel('Length (Tokens)');\n",
        "plt.tight_layout();\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndIBffxybcyV"
      },
      "outputs": [],
      "source": [
        "import traceback\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "max_input_length = model.config.max_position_embeddings # BART\n",
        "print(f'BART model maximum INPUT length: { max_input_length }')\n",
        "max_target_length = 100 # manually set\n",
        "assert max_target_length <= max_input_length\n",
        "print(f'Manually set maximum TARGET length: { max_target_length }')\n",
        "\n",
        "# Model will have minimum <stride> context tokens when calculating conditional likelihood of a token\n",
        "# (Provided there are <stride> preceding tokens available to condition on)\n",
        "# These tokens are set to -100 so that they're ignored for the loss calculation\n",
        "stride = 512\n",
        "\n",
        "all_out = list()\n",
        "num_examples = 10\n",
        "\n",
        "for article, ref_summary in zip(tqdm(test_articles[:num_examples]), tqdm(test_summaries[:num_examples])):\n",
        "\t# Initialize hash, counter\n",
        "\ttmp = dict()\n",
        "\n",
        "\t# Convert source text to model input IDs\n",
        "\tencodings_source = tokenizer(\n",
        "\t\ttext=article,\n",
        "\t\tmax_length=max_input_length-2, # subtract 2 because we want to add BOS and EOS tokens\n",
        "\t\ttruncation=True,\n",
        "\t\treturn_offsets_mapping=True\n",
        "\t\t# padding=True\n",
        "\t)\n",
        "\t# no need to add BOS and EOS since tokenizer does that already\n",
        "\tinput_ids = torch.tensor(encodings_source['input_ids'], device=model.device)\n",
        "\tinput_ids = input_ids.unsqueeze(dim=0) # BART model needs tensor([[]]), encodings alone return tensor([])\n",
        "\tprint(f'Input IDs (BOS + encodings + EOS): { input_ids.size() }')\n",
        "\n",
        "\t# Convert target text to model decoder input IDs and labels\n",
        "\tencodings_target = tokenizer(\n",
        "\t\ttext=ref_summary,\n",
        "\t\tmax_length=max_target_length-1, # subtract 1 because we want to add BOS/EOS token\n",
        "\t\ttruncation=True,\n",
        "\t\treturn_offsets_mapping=True,\n",
        "\t)\n",
        "\tlabel_ids = torch.tensor(encodings_target['input_ids'], device=model.device)\n",
        "\tlabel_ids = label_ids.unsqueeze(dim=0)\n",
        "\tprint(f'Label IDs (BOS + encodings + EOS): { label_ids.size() }')\n",
        "\n",
        "\t# decoder_input_ids = torch.tensor(\n",
        "\t# \tdata=[tokenizer.bos_token_id]+encodings_target['input_ids'],\n",
        "\t# \tdevice=model.device\n",
        "\t# )\n",
        "\t# decoder_input_ids = decoder_input_ids.unsqueeze(dim=0)\n",
        "\t# print(f'Decoder inputs: { decoder_input_ids.size() }')\n",
        "\t# labels = torch.tensor(\n",
        "\t# \tdata=encodings_target['input_ids']+[tokenizer.eos_token_id],\n",
        "\t# \tdevice=model.device\n",
        "\t# )\n",
        "\t# labels = labels.unsqueeze(dim=0)\n",
        "\t# print(f'Labels: { labels.size() }')\n",
        "\n",
        "\twith torch.no_grad():\n",
        "\t\toutput = model( # not using model.generate since it doesn't support loss calculation\n",
        "\t\t\tinput_ids=input_ids,\n",
        "\t\t\t# decoder_input_ids=decoder_input_ids,\n",
        "\t\t\tlabels=label_ids\n",
        "\t\t)\n",
        "\t\n",
        "\t# logit(p) = log(p/(1-p)) transforms probability values => domain [0, 1] range [-inf, +inf]\n",
        "\t# unlike LM, don't need to shift logits for AS\n",
        "\tlogits = output['logits'].squeeze()\n",
        "\n",
        "\t# labels = ground truths taken from reference summary\n",
        "\t# thus, don't need to shift labels\n",
        "\tlabel_ids = label_ids.contiguous().squeeze(dim=0)\n",
        "\n",
        "\t# NLL = multi-class cross entropy (in this case) --> calculated in log-e\n",
        "\t# CE loss: H(P, Q) = -\\sum(x in X) P(x) \\log_{2}{Q(x)}, where Q(x) is the estimated distribution\n",
        "\t# Note: this calculation is equivalent to calculating probs * -log_probs, then for each token, select column corresponding to vocab index (see sanity check below)\n",
        "\t# Equivalent to IC: higher P(w) --> lower IC (relatively little info gained)\n",
        "\tce_loss = torch.nn.functional.cross_entropy( # input ids x 1\n",
        "\t\tinput=logits.view(-1, logits.size(-1)), # Predicted unnormalized logits\n",
        "\t\ttarget=label_ids.view(-1), # Ground truth class labels (one-hot encoded)\n",
        "\t\treduction='none' # No reduction applied to cross entropy output\n",
        "\t)\n",
        "\n",
        "\t# Sanity check: mean NLL should be close to automatic loss calculation\n",
        "\t# print(f'Mean NLL: { torch.mean(ce_loss) }')\n",
        "\t# print(f'Automatic loss calculation: { output[\"loss\"] }')\n",
        "\tassert torch.isclose(\n",
        "\t\tinput=torch.mean(ce_loss),\n",
        "\t\tother=output['loss'],\n",
        "\t\trtol=0.1\n",
        "\t)\n",
        "\n",
        "\t# Softmax to normalize logits\n",
        "\tprobs = logits.softmax(dim=-1) # input ids x vocabulary\n",
        "\tprint(f'Probs { probs.size() }:\\n{ probs }')\n",
        "\ttoken_probs = torch.tensor([prob[vocab_idx] for vocab_idx, prob in zip(label_ids, probs)]) # input ids x 1\n",
        "\tprint(f'Token probs { token_probs.size() }:\\n{ token_probs }')\n",
        "\n",
        "\t# Sanity check: probabilities across each encoding (row) sums to 1\n",
        "\tfor idx, vocab_token in enumerate(probs):\n",
        "\t\tassert torch.isclose(\n",
        "\t\t\tinput=torch.sum(vocab_token),\n",
        "\t\t\tother=torch.tensor(1.),\n",
        "\t\t\trtol=0.01\n",
        "\t\t)\n",
        "\t# Sanity check: equivalently, sum of all elements in probabilities tensor should equal number of tokens\n",
        "\tsum_probs = torch.sum(probs, dtype=torch.float32)\n",
        "\tlen_labels = torch.tensor([label_ids.size()[0]], dtype=torch.float32)\n",
        "\t# print(f'Sum of all elements in probabilities tensor: { sum_probs }')\n",
        "\t# print(f'Number of tokens: { len_labels }')\n",
        "\tassert torch.isclose(\n",
        "\t\tinput=sum_probs,\n",
        "\t\tother=len_labels,\n",
        "\t\trtol=0.01\n",
        "\t)\n",
        "\n",
        "\t# Sanity check: NLL of token at token index equals CE loss\n",
        "\tnlls = -1 * logits.log_softmax(dim=-1) # input ids x vocabularly\n",
        "\tics = torch.tensor([nll[vocab_idx] for vocab_idx, nll in zip(label_ids, nlls)]) # input ids x 1\n",
        "\tassert torch.allclose(\n",
        "\t\tinput=ics,\n",
        "\t\tother=ce_loss\n",
        "\t)\n",
        "\t\n",
        "\t# Entropy (a.k.a. expected IC): sum_{v in vocab} p(v) log p(v) at each token position (sum along row)\n",
        "\tper_token_entropy = probs * nlls # input ids x vocabulary\n",
        "\tper_token_entropy = torch.sum(per_token_entropy, dim=1) # input ids x 1, dim=1 means sum along each row (token)\n",
        "\n",
        "\t# Sanity check: number of entropies calculated equals number of summary tokens\n",
        "\tprint(f'Expected: { torch.tensor(label_ids.size()) }')\n",
        "\tprint(f'Actual: { torch.tensor(per_token_entropy.size()) }\\n')\n",
        "\tassert torch.tensor(per_token_entropy.size()) == torch.tensor(label_ids.size())\n",
        "\t\n",
        "\ttmp['article'] = article\n",
        "\ttmp['ref_summary'] = ref_summary\n",
        "\ttmp['tensor_input'] = input_ids\n",
        "\ttmp['logits'] = logits\n",
        "\ttmp['labels'] = label_ids\n",
        "\ttmp['probs'] = token_probs # probs but at the correct indices => input ids x 1\n",
        "\ttmp['nlls'] = nlls # input ids x vocabulary\n",
        "\ttmp['ics'] = ics # nlls but at the correct indices (equivalently ce_loss and per token NLL) => input ids x 1\n",
        "\ttmp['per_token_entropy'] = per_token_entropy\n",
        "\n",
        "\tall_out.append(tmp)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "K00kF24GbkfW"
      },
      "source": [
        "### Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwqrdVMgbkfW"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMVTDAEDbkfX",
        "outputId": "08ade9b7-1980-4631-a565-2d09326003c4"
      },
      "outputs": [],
      "source": [
        "print(f'Number of articles summarized: { len(all_out) }')\n",
        "\n",
        "all_token_probs = torch.cat([text['probs'] for text in all_out])\n",
        "all_ics = torch.cat([text['ics'] for text in all_out])\n",
        "all_entropies = torch.cat([text['per_token_entropy'] for text in all_out])\n",
        "assert all_ics.size() == all_entropies.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BquV6snfbkfX",
        "outputId": "480d77ac-0463-4b10-d371-4d161071fac6"
      },
      "outputs": [],
      "source": [
        "# Perplexity\n",
        "mean_nll = torch.mean(all_ics)\n",
        "perplexity = torch.exp(mean_nll)\n",
        "print(f'Perplexity: { perplexity }')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "deyGVz5obkfX",
        "outputId": "6bf194aa-1a0c-4a0c-8bf3-3b1755e087c7"
      },
      "outputs": [],
      "source": [
        "#### NLL & ENTROPY\n",
        "## Density plot\n",
        "fig1, (ics, entropies) = plt.subplots(1, 2, sharex=False, sharey=False);\n",
        "fig1.suptitle('Token attributes');\n",
        "\n",
        "# Information content (NLL)\n",
        "sns.kdeplot(\n",
        "\tax=ics,\n",
        "\tdata=all_ics,\n",
        "\tbw_method=0.5,\n",
        "\tcolor='green',\n",
        "\tfill=True,\n",
        "\tlabel='IC'\n",
        ");\n",
        "ics.set(title='Negative log likelihood');\n",
        "ics.set(xlabel='Bits');\n",
        "\n",
        "# Entropy (expected IC)\n",
        "sns.kdeplot(\n",
        "\tax=entropies,\n",
        "\tdata=all_entropies,\n",
        "\tbw_method=0.5,\n",
        "\tcolor='blue',\n",
        "\tfill=True,\n",
        "\tlabel='Entropy'\n",
        ");\n",
        "entropies.set(title='Entropy');\n",
        "entropies.set(xlabel='Bits');\n",
        "# entropies.xaxis.set_major_locator(ticker.MultipleLocator(2));\n",
        "# entropies.xaxis.set_major_formatter(ticker.ScalarFormatter());\n",
        "\n",
        "plt.show();\n",
        "\n",
        "# abs(diff)\n",
        "diff_ic_ent = torch.sub(all_ics, all_entropies);\n",
        "\n",
        "abs_diff_ic_ent = torch.abs(diff_ic_ent);\n",
        "\n",
        "density_abs_diff_ic_ent = sns.kdeplot(\n",
        "\tdata=abs_diff_ic_ent,\n",
        "\tbw_method=1,\n",
        "\tcolor='orange',\n",
        "\tfill=True,\n",
        "\tlabel='abs. diff'\n",
        "\t# clip=(0, 100)\n",
        ");\n",
        "\n",
        "# sq(diff)\n",
        "sq_diff_ic_ent = torch.square(diff_ic_ent)\n",
        "density_sq_diff_ic_ent = sns.kdeplot(\n",
        "\tdata=sq_diff_ic_ent,\n",
        "\tbw_method=1,\n",
        "\tcolor='yellow',\n",
        "\tfill=True,\n",
        "\tlabel='sq. diff'\n",
        "\t# clip=(0, 100)\n",
        ");\n",
        "\n",
        "xmax = max(sq_diff_ic_ent);\n",
        "density_sq_diff_ic_ent.set_title('Deviation of IC from expected IC');\n",
        "density_sq_diff_ic_ent.legend(loc=0);\n",
        "density_sq_diff_ic_ent.set(xlim=(0, xmax));\n",
        "density_sq_diff_ic_ent.set(xlabel='Bits');\n",
        "\n",
        "plt.show();\n",
        "\n",
        "print(f'Mean abs diff: { torch.mean(abs_diff_ic_ent) }');\n",
        "print(f'Median abs diff: { torch.median(abs_diff_ic_ent) }');\n",
        "print(f'Max abs diff: { torch.max(abs_diff_ic_ent) }');\n",
        "print(f'Min abs diff: { torch.min(abs_diff_ic_ent) }');"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oCvlGaqKFD64"
      },
      "source": [
        "### **Story generation** with GPT and ```WritingPrompts```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snbBsfkQFD64",
        "outputId": "a689b242-cd54-4c4e-bb3e-ba7d80ea81b1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
        "\n",
        "# Load model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using device: { device.upper() }')\n",
        "model_checkpoint = 'gpt2'\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(model_checkpoint)\n",
        "if tokenizer.pad_token is None:\n",
        "\ttokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "model = GPT2LMHeadModel.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5jw7UTUFD64",
        "outputId": "cf82f168-7e84-4540-d7f9-56a42363dfc0"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "import pandas as pd\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "prompts = pd.read_fwf('../data/writingPrompts/test.wp_source', header=None)[0]\n",
        "# prompts = pd.read_fwf('/content/drive/MyDrive/ColabNotebooks/data/writingPrompts/test.wp_source', header=None)[0]\n",
        "prompts.replace('.*] ', '', regex=True, inplace=True)\n",
        "prompts.replace('.?<newline>.?', ' ', regex=True, inplace=True) # or keep?\n",
        "\n",
        "ref_stories = pd.read_fwf('../data/writingPrompts/test.wp_target', header=None)[0]\n",
        "# ref_stories = pd.read_fwf('/content/drive/MyDrive/ColabNotebooks/data/writingPrompts/test.wp_target', header=None)[0]\n",
        "ref_stories.replace('.?<newline>.?', ' ', regex=True, inplace=True) # or keep?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKH_kMqkFD64",
        "outputId": "f4d4c0b3-43b6-417f-a6f1-64e841b756ee"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "max_input_length = model.config.n_positions # GPT2\n",
        "print(f'GPT2 model maximum input length: { max_input_length }')\n",
        "\n",
        "\n",
        "# Model will have minimum <stride> context tokens when calculating conditional likelihood of a token\n",
        "# (Provided there are <stride> preceding tokens available to condition on)\n",
        "# These tokens are set to -100 so that they're ignored for the loss calculation\n",
        "stride = 512\n",
        "\n",
        "all_out = list()\n",
        "num_examples = 10\n",
        "\n",
        "for prompt, ref_story in zip(tqdm(prompts[:num_examples]), tqdm(ref_stories[:num_examples])):\n",
        "\t# Initialize hash, counter\n",
        "\ttmp = dict()\n",
        "\n",
        "\t# Concatenate prompt and reference story\n",
        "\tinput_text = prompt + ref_story\n",
        "\tif len(input_text.split()) > max_input_length:\n",
        "\t\tcontinue\n",
        "\n",
        "\t# Convert text to model input IDs\n",
        "\tencodings = tokenizer(\n",
        "\t\ttext=input_text,\n",
        "\t\tmax_length=max_input_length-2, # subtract 2 because we want to add BOS and EOS tokens\n",
        "\t\ttruncation=True,\n",
        "\t\treturn_offsets_mapping=True # for fast tokenizers only\n",
        "\t)\n",
        "\tinput_ids = torch.tensor(\n",
        "\t\t# BOS token + encodings + EOS token\n",
        "\t\tdata=[tokenizer.bos_token_id]+encodings['input_ids']+[tokenizer.eos_token_id],\n",
        "\t\tdevice=model.device\n",
        "\t)\n",
        "\n",
        "\twith torch.no_grad():\n",
        "\t\toutput = model(\n",
        "\t\t\tinput_ids=input_ids,\n",
        "\t\t\tlabels=input_ids\n",
        "\t\t)\n",
        "\t\n",
        "\t# logit(p) = log(p/(1-p)) transforms probability values => domain [0, 1] range [-inf, +inf]\n",
        "\t# shift logits: up to index -1 because we don't care about probability distribution over tokens after EOS\n",
        "\tlogits_shifted = output['logits'][..., :-1, :].contiguous() # input ids x vocabulary\n",
        "\n",
        "\t# labels = ground truths taken from <tensor_input>\n",
        "\t# originally, logits[0] is the distribution conditioned on labels[0], but for logits we actually care about the probability conditioned on the preceding token\n",
        "\t# (as consistent with language modeling: what is the probability of token x_t given x_<t?)\n",
        "\t# => shift labels: align position 1 of the labels with position 0 of the logits => logits[0] should correspond to labels[1]\n",
        "\tlabels_shifted = input_ids[..., 1:].contiguous().squeeze() # input ids x 1\n",
        "\n",
        "\t# NLL = multi-class cross entropy (in this case) --> calculated in log-e\n",
        "\t# CE loss: H(P, Q) = -\\sum(x in X) P(x) \\log_{2}{Q(x)}, where Q(x) is the estimated distribution\n",
        "\t# Note: this calculation is equivalent to calculating probs * -1og_probs, then for each token, select column corresponding to vocab index (see sanity check below)\n",
        "\t# Equivalent to IC: higher P(w) --> lower IC (relatively little info gained)\n",
        "\tce_loss = torch.nn.functional.cross_entropy( # input ids x 1\n",
        "\t\tinput=logits_shifted.view(-1, logits_shifted.size(-1)), # Predicted unnormalized logits\n",
        "\t\ttarget=labels_shifted.view(-1), # Ground truth class labels (one-hot encoded)\n",
        "\t\treduction='none' # No reduction applied to cross entropy output\n",
        "\t)\n",
        "\t\n",
        "\t# Sanity check: mean NLL (CE loss) should be close to automatic loss calculation\n",
        "\t# print(f'Mean NLL: { torch.mean(ce_loss) }')\n",
        "\t# print(f'Automatic loss calculation: { output[\"loss\"] }')\n",
        "\tassert torch.isclose(\n",
        "\t\tinput=torch.mean(ce_loss),\n",
        "\t\tother=output['loss'],\n",
        "\t\trtol=0.1\n",
        "\t)\n",
        "\t\n",
        "\t# Softmax to normalize logits\n",
        "\tprobs = logits_shifted.softmax(dim=-1).squeeze() # input ids x vocabulary\n",
        "\ttoken_probs = torch.tensor([prob[vocab_idx] for vocab_idx, prob in zip(labels_shifted, probs)]) # probs for the correct indices (i.e. those corresponding to the tokens) => input ids x 1\n",
        "\t\n",
        "\t# Sanity check: probabilities across each encoding (row) sums to 1\n",
        "\tfor idx, vocab_token in enumerate(probs):\n",
        "\t\tassert torch.isclose(\n",
        "\t\t\tinput=torch.sum(vocab_token),\n",
        "\t\t\tother=torch.tensor(1.),\n",
        "\t\t\trtol=0.01\n",
        "\t\t)\n",
        "\t# Equivalently, sum of all elements in probabilities tensor should equal number of tokens\n",
        "\tsum_probs = torch.sum(probs, dtype=torch.float32)\n",
        "\tlen_labels = torch.tensor([labels_shifted.size()[0]], dtype=torch.float32)\n",
        "\t# print(f'Sum of all elements in probabilities tensor: { sum_probs }')\n",
        "\t# print(f'Number of tokens: { len_labels }')\n",
        "\tassert torch.isclose(\n",
        "\t\tinput=sum_probs,\n",
        "\t\tother=len_labels,\n",
        "\t\trtol=0.01\n",
        "\t)\n",
        "\n",
        "\t# Sanity check: NLL of token at token index equals CE loss\n",
        "\tnlls = -1 * logits_shifted.log_softmax(dim=-1).squeeze() # input ids x vocabulary\n",
        "\t# print(f'labels tensor (size {labels_shifted.size()}):\\n{labels_shifted}')\n",
        "\t# print(f'NLLs tensor (size {nlls.size()}):\\n{nlls}')\n",
        "\t# for vocab_idx, prob in zip(labels_shifted, probs):\n",
        "\t# \tprint(f'vocab index {vocab_idx}: {prob[vocab_idx]}')\n",
        "\tics = torch.tensor([nll[vocab_idx] for vocab_idx, nll in zip(labels_shifted, nlls)]) # input ids x 1\n",
        "\tassert torch.allclose(\n",
        "\t\t\t\tinput=ics,\n",
        "\t\t\t\tother=ce_loss\n",
        "\t\t\t)\n",
        "\n",
        "\t# Entropy (a.k.a. expected IC): sum_{v in vocab} p(v) log p(v) at each token position (sum along row)\n",
        "\tper_token_entropy = probs * nlls # input ids x vocabulary\n",
        "\tper_token_entropy = torch.sum(per_token_entropy, dim=1) # input ids x 1, dim=1 means sum along each row\n",
        "\n",
        "\t# Sanity check: number of entropies calculated equals number of tokens minus 1 (shifted logits and labels)\n",
        "\t# print(f'Expected: { torch.tensor(labels.size())[1] - torch.tensor([1.]) }')\n",
        "\t# print(f'Actual: { torch.tensor(per_token_entropy.size()) }\\n')\n",
        "\tassert torch.tensor(per_token_entropy.size()) == torch.tensor(input_ids.size()) - torch.tensor([1.])\n",
        "\n",
        "\ttmp['prompt'] = prompt\n",
        "\ttmp['ref_story'] = ref_story\n",
        "\ttmp['full_input'] = input_text\n",
        "\ttmp['len_full_input'] = input_ids.size()[0]\n",
        "\ttmp['tensor_input'] = input_ids\n",
        "\ttmp['logits'] = logits_shifted.squeeze()\n",
        "\ttmp['labels'] = labels_shifted\n",
        "\ttmp['probs'] = token_probs # probs but at the correct indices => input ids x 1\n",
        "\ttmp['nlls'] = nlls # input ids x vocabulary\n",
        "\ttmp['ics'] = ics # nlls but at the correct indices (equivalently ce_loss and per token NLL) => input ids x 1\n",
        "\ttmp['per_token_entropy'] = per_token_entropy\n",
        "\n",
        "\tall_out.append(tmp)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ISJSn0x7FD64"
      },
      "source": [
        "### Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsknNr5tFD65"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "sns.set() # Set sns as default style"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeGE6ENcFD65",
        "outputId": "9d619a0f-6921-452f-a355-5d6912000d54"
      },
      "outputs": [],
      "source": [
        "print(f'Number of texts processed: { len(all_out) }')\n",
        "\n",
        "all_token_probs = torch.cat([text['probs'] for text in all_out])\n",
        "all_ics = torch.cat([text['ics'] for text in all_out])\n",
        "all_entropies = torch.cat([text['per_token_entropy'] for text in all_out])\n",
        "assert all_ics.size() == all_entropies.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwykKficFD65",
        "outputId": "3d7bcaf0-8011-4de2-8a59-4ab4a1e7ad5a"
      },
      "outputs": [],
      "source": [
        "# Perplexity\n",
        "mean_nll = torch.mean(all_ics)\n",
        "perplexity = torch.exp(mean_nll)\n",
        "print(f'Perplexity: { perplexity }')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "H7X1qe_LFD65",
        "outputId": "e473cc98-dc1c-491e-87e7-1e75009ea166"
      },
      "outputs": [],
      "source": [
        "#### NLL & ENTROPY\n",
        "## Density plot\n",
        "fig1, (ics, entropies) = plt.subplots(1, 2, sharex=True, sharey=True);\n",
        "fig1.suptitle('Token attributes');\n",
        "\n",
        "# Information content (NLL)\n",
        "sns.kdeplot(\n",
        "\tax=ics,\n",
        "\tdata=all_ics,\n",
        "\tbw_method=0.5,\n",
        "\tcolor='green',\n",
        "\tfill=True,\n",
        "\tlabel='IC'\n",
        ");\n",
        "ics.set(title='Negative log likelihood');\n",
        "ics.set(xlabel='Bits');\n",
        "ics.set(xlim=(0));\n",
        "ics.xaxis.set_major_locator(ticker.MultipleLocator(2));\n",
        "ics.xaxis.set_major_formatter(ticker.ScalarFormatter());\n",
        "\n",
        "# Entropy (expected IC)\n",
        "sns.kdeplot(\n",
        "\tax=entropies,\n",
        "\tdata=all_entropies,\n",
        "\tbw_method=0.5,\n",
        "\tcolor='blue',\n",
        "\tfill=True,\n",
        "\tlabel='Entropy'\n",
        ");\n",
        "entropies.set(title='Entropy');\n",
        "entropies.set(xlabel='Bits');\n",
        "# entropies.set(xlim=(0));\n",
        "entropies.xaxis.set_major_locator(ticker.MultipleLocator(2));\n",
        "entropies.xaxis.set_major_formatter(ticker.ScalarFormatter());\n",
        "\n",
        "plt.show();\n",
        "\n",
        "\n",
        "#### DIFF(NLL, ENTROPY)\n",
        "diff_ic_ent = torch.sub(all_ics, all_entropies);\n",
        "# density_diff_ic_ent = sns.kdeplot(\n",
        "# \tdata=diff_ic_ent,\n",
        "# \tbw_method=1,\n",
        "# \tcolor='red',\n",
        "# \tfill=True,\n",
        "# \tlabel='diff'\n",
        "# \t# clip=(0, 100)\n",
        "# )\n",
        "# density_diff_ic_ent.set(\n",
        "# \ttitle='diff(IC, entropy)',\n",
        "# \txlabel='Bits',\n",
        "# \tylabel='Density'\n",
        "# )\n",
        "\n",
        "# abs(diff)\n",
        "abs_diff_ic_ent = torch.abs(diff_ic_ent);\n",
        "\n",
        "density_abs_diff_ic_ent = sns.kdeplot(\n",
        "\tdata=abs_diff_ic_ent,\n",
        "\tbw_method=1,\n",
        "\tcolor='orange',\n",
        "\tfill=True,\n",
        "\tlabel='abs. diff'\n",
        "\t# clip=(0, 100)\n",
        ");\n",
        "\n",
        "# sq(diff)\n",
        "sq_diff_ic_ent = torch.square(diff_ic_ent)\n",
        "density_sq_diff_ic_ent = sns.kdeplot(\n",
        "\tdata=sq_diff_ic_ent,\n",
        "\tbw_method=1,\n",
        "\tcolor='yellow',\n",
        "\tfill=True,\n",
        "\tlabel='sq. diff'\n",
        "\t# clip=(0, 100)\n",
        ");\n",
        "\n",
        "xmax = max(sq_diff_ic_ent)+1;\n",
        "density_sq_diff_ic_ent.set_title('Deviation of IC from expected IC');\n",
        "density_sq_diff_ic_ent.legend(loc=0);\n",
        "density_sq_diff_ic_ent.set(xlim=(0, xmax));\n",
        "density_sq_diff_ic_ent.set(xlabel='Bits');\n",
        "\n",
        "plt.show();\n",
        "\n",
        "print(f'Mean abs diff: { torch.mean(abs_diff_ic_ent) }');\n",
        "print(f'Median abs diff: { torch.median(abs_diff_ic_ent) }');\n",
        "print(f'Max abs diff: { torch.max(abs_diff_ic_ent) }');\n",
        "print(f'Min abs diff: { torch.min(abs_diff_ic_ent) }');"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_JIhYq17UBiI"
      },
      "source": [
        "### <strong>Story generation</strong> with BART and ```WritingPrompts```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSVyY0P-UCIs",
        "outputId": "7bfb8204-c4ba-4827-d68b-95b6e82864ca"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
        "\n",
        "# Load model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using device: { device.upper() }')\n",
        "model_checkpoint = 'facebook/bart-large-cnn'\n",
        "tokenizer = BartTokenizer.from_pretrained(model_checkpoint)\n",
        "if tokenizer.pad_token is None:\n",
        "\ttokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "model = BartForConditionalGeneration(BartConfig())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "au0qXwmfrxwO",
        "outputId": "a876ec7d-f28d-4d92-bba2-b0b026719630"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "import pandas as pd\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "prompts = pd.read_fwf('../data/writingPrompts/test.wp_source', header=None)[0]\n",
        "# prompts = pd.read_fwf('/content/drive/MyDrive/ColabNotebooks/data/writingPrompts/test.wp_source', header=None)[0]\n",
        "prompts.replace('.*] ', '', regex=True, inplace=True)\n",
        "prompts.replace('.?<newline>.?', ' ', regex=True, inplace=True) # or keep?\n",
        "\n",
        "ref_stories = pd.read_fwf('../data/writingPrompts/test.wp_target', header=None)[0]\n",
        "# ref_stories = pd.read_fwf('/content/drive/MyDrive/ColabNotebooks/data/writingPrompts/test.wp_target', header=None)[0]\n",
        "ref_stories.replace('.?<newline>.?', ' ', regex=True, inplace=True) # or keep?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuhyQ0pke53T",
        "outputId": "c791eea2-36ab-480a-be3d-d1e9b3102c6a"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "max_input_length = model.config.max_position_embeddings # BART\n",
        "print(f'BART model maximum INPUT length: { max_input_length }')\n",
        "\n",
        "# Model will have minimum <stride> context tokens when calculating conditional likelihood of a token\n",
        "# (Provided there are <stride> preceding tokens available to condition on)\n",
        "# These tokens are set to -100 so that they're ignored for the loss calculation\n",
        "stride = 512\n",
        "\n",
        "all_out = list()\n",
        "num_examples = 10\n",
        "\n",
        "for prompt, target in zip(tqdm(prompts[:num_examples]), tqdm(ref_stories[:num_examples])):\n",
        "\t# Initialize hash, counter\n",
        "\ttmp = dict()\n",
        "\n",
        "\t# Convert prompt to model input IDs\n",
        "\tencodings_prompt = tokenizer(\n",
        "\t\ttext=prompt,\n",
        "\t\tmax_length=max_input_length-1, # subtract 1 because we want to add BOS token\n",
        "\t\ttruncation=True\n",
        "\t\t# return_offsets_mapping=True # for fast tokenizers only\n",
        "\t)\n",
        "\tinput_ids = torch.tensor(\n",
        "\t\t# want prompt to start with BOS but not end with EOS\n",
        "\t\tdata=encodings_prompt['input_ids'][:-1],\n",
        "\t\tdevice=model.device\n",
        "\t)\n",
        "\tinput_ids = input_ids.unsqueeze(dim=0) # Bart model needs tensor([[]]), input alone returns only tensor([])\n",
        "\n",
        "\t# Convert reference story to label IDs\n",
        "\tencodings_target = tokenizer(\n",
        "\t\ttext=target,\n",
        "\t\tmax_length=max_input_length-1, # subtract 1 because we want to keep EOS token\n",
        "\t\ttruncation=True,\n",
        "\t\tpadding=True\n",
        "\t)\n",
        "\tlabels = torch.tensor(\n",
        "\t\t# want generated story to end with EOS but not start with BOS\n",
        "\t\tdata=encodings_target['input_ids'][1:],\n",
        "\t\tdevice=model.device\n",
        "\t)\n",
        "\tlabels = labels.unsqueeze(dim=0)\n",
        "\n",
        "\twith torch.no_grad():\n",
        "\t\toutput = model(\n",
        "\t\t\tinput_ids=input_ids,\n",
        "\t\t\tlabels=labels\n",
        "\t\t)\n",
        "\tloss = output['loss']\n",
        "\t# print(f'model loss: { loss }')\n",
        "\t\n",
        "\t# logit(p) = log(p/(1-p)) transforms probability values => domain [0, 1] range [-inf, +inf]\n",
        "\t# shift logits: up to index -1 because we don't care about probability distribution over tokens after EOS\n",
        "\tlogits = output['logits'].squeeze(dim=0) # input ids x vocabulary\n",
        "\n",
        "\t# labels = ground truths taken from reference summary\n",
        "\t# thus, don't need to shift labels\n",
        "\tlabels = labels.contiguous().squeeze(dim=0)\n",
        "\n",
        "\t# NLL = multi-class cross entropy (in this case) --> calculated in log-e\n",
        "\t# CE loss: H(P, Q) = -\\sum(x in X) P(x) \\log_{2}{Q(x)}, where Q(x) is the estimated distribution\n",
        "\t# Note: this calculation is equivalent to calculating probs * -1og_probs, then for each token, select column corresponding to vocab index (see sanity check below)\n",
        "\t# Equivalent to IC: higher P(w) --> lower IC (relatively little info gained)\n",
        "\tce_loss = torch.nn.functional.cross_entropy( # input ids x 1\n",
        "\t\tinput=logits.view(-1, logits.size(-1)), # Predicted unnormalized logits\n",
        "\t\ttarget=labels.view(-1), # Ground truth class labels (one-hot encoded)\n",
        "\t\treduction='none' # No reduction applied to cross entropy output\n",
        "\t)\n",
        "\t# print(f'mean CE loss: { torch.mean(ce_loss) }')\n",
        "\t\n",
        "\t# Sanity check: mean NLL (CE loss) should be close to automatic loss calculation\n",
        "\t# print(f'Mean NLL: { torch.mean(ce_loss) }')\n",
        "\t# print(f'Automatic loss calculation: { output[\"loss\"] }')\n",
        "\tassert torch.isclose(\n",
        "\t\tinput=torch.mean(ce_loss),\n",
        "\t\tother=output['loss'],\n",
        "\t\trtol=0.1\n",
        "\t)\n",
        "\t\n",
        "\t# Softmax to normalize logits\n",
        "\tlabels = labels.squeeze(dim=0)\n",
        "\tprobs = logits.softmax(dim=-1).squeeze(dim=0) # input ids x vocabulary\n",
        "\ttoken_probs = torch.tensor([prob[vocab_idx] for vocab_idx, prob in zip(labels, probs)]) # probs for the correct indices (i.e. those corresponding to the tokens) => input ids x 1\n",
        "\t\n",
        "\t# Sanity check: probabilities across each encoding (row) sums to 1\n",
        "\tfor idx, vocab_token in enumerate(probs):\n",
        "\t\tassert torch.isclose(\n",
        "\t\t\tinput=torch.sum(vocab_token),\n",
        "\t\t\tother=torch.tensor(1.),\n",
        "\t\t\trtol=0.01\n",
        "\t\t)\n",
        "\t# Sanity check: equivalently, sum of all elements in probabilities tensor should equal number of tokens\n",
        "\tsum_probs = torch.sum(probs, dtype=torch.float32)\n",
        "\tlen_labels = torch.tensor([labels.size()[0]], dtype=torch.float32)\n",
        "\t# print(f'Sum of all elements in probabilities tensor: { sum_probs }')\n",
        "\t# print(f'Number of tokens: { len_labels }')\n",
        "\tassert torch.isclose(\n",
        "\t\tinput=sum_probs,\n",
        "\t\tother=len_labels,\n",
        "\t\trtol=0.01\n",
        "\t)\n",
        "\n",
        "\t# Sanity check: NLL of token at token index equals CE loss\n",
        "\tnlls = -1 * logits.log_softmax(dim=-1).squeeze() # input ids x vocabulary\n",
        "\tics = torch.tensor([nll[vocab_idx] for vocab_idx, nll in zip(labels, nlls)]) # input ids x 1\n",
        "\tassert torch.allclose(\n",
        "\t\t\t\tinput=ics,\n",
        "\t\t\t\tother=ce_loss\n",
        "\t\t\t)\n",
        "\n",
        "\t# Entropy (a.k.a. expected IC): sum_{v in vocab} p(v) log p(v) at each token position (sum along row)\n",
        "\tper_token_entropy = probs * nlls # input ids x vocabulary\n",
        "\tper_token_entropy = torch.sum(per_token_entropy, dim=1) # input ids x 1, dim=1 means sum along each row\n",
        "\n",
        "\t# Sanity check: number of entropies calculated equals number of summary tokens minus 1 (shifted logits and labels)\n",
        "\t# print(f'Expected: { torch.tensor(labels.size()) }')\n",
        "\t# print(f'Actual: { torch.tensor(per_token_entropy.size()) }\\n')\n",
        "\tassert torch.tensor(per_token_entropy.size()) == torch.tensor(labels.size())\n",
        "\n",
        "\ttmp['prompt'] = prompt\n",
        "\ttmp['ref_story'] = target\n",
        "\ttmp['len_prompt'] = input_ids.size()[0] - labels.size()[0]\n",
        "\ttmp['len_ref_story'] = labels.size()[0]\n",
        "\ttmp['tensor_input'] = input_ids.squeeze()\n",
        "\ttmp['logits'] = logits\n",
        "\ttmp['labels'] = labels\n",
        "\ttmp['probs'] = token_probs # probs but at the correct indices => input ids x 1\n",
        "\ttmp['nlls'] = nlls # input ids x vocabulary\n",
        "\ttmp['ics'] = ics # nlls but at the correct indices (equivalently ce_loss and per token NLL) => input ids x 1\n",
        "\ttmp['per_token_entropy'] = per_token_entropy\n",
        "\n",
        "\tall_out.append(tmp)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ams3120jFD63"
      },
      "source": [
        "### Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_EJHVq3FD63"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "sns.set() # Set sns as default style"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ct-Aye27FD63",
        "outputId": "746677be-cbad-4d08-8493-78319b4ba771"
      },
      "outputs": [],
      "source": [
        "print(f'Number of texts processed: { len(all_out) }')\n",
        "\n",
        "all_token_probs = torch.cat([text['probs'] for text in all_out])\n",
        "all_ics = torch.cat([text['ics'] for text in all_out])\n",
        "all_entropies = torch.cat([text['per_token_entropy'] for text in all_out])\n",
        "assert all_ics.size() == all_entropies.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrRumfMYFD64",
        "outputId": "83bc76ba-5ae3-4f1c-de54-64290bcd3376"
      },
      "outputs": [],
      "source": [
        "# Perplexity\n",
        "mean_nll = torch.mean(all_ics)\n",
        "perplexity = torch.exp(mean_nll)\n",
        "print(f'Perplexity: { perplexity }')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "Un95lb_OFD64",
        "outputId": "09e9369d-2194-49c0-8265-1f3a377145ff"
      },
      "outputs": [],
      "source": [
        "#### NLL & ENTROPY\n",
        "## Density plot\n",
        "fig1, (ics, entropies) = plt.subplots(1, 2, sharex=False, sharey=False);\n",
        "fig1.suptitle('Token attributes');\n",
        "\n",
        "# Information content (NLL)\n",
        "sns.kdeplot(\n",
        "\tax=ics,\n",
        "\tdata=all_ics,\n",
        "\tbw_method=0.5,\n",
        "\tcolor='green',\n",
        "\tfill=True,\n",
        "\tlabel='IC'\n",
        ");\n",
        "ics.set(title='Negative log likelihood');\n",
        "ics.set(xlabel='Bits');\n",
        "ics.set(xlim=(0));\n",
        "ics.xaxis.set_major_locator(ticker.MultipleLocator(2));\n",
        "ics.xaxis.set_major_formatter(ticker.ScalarFormatter());\n",
        "\n",
        "# Entropy (expected IC)\n",
        "sns.kdeplot(\n",
        "\tax=entropies,\n",
        "\tdata=all_entropies,\n",
        "\tbw_method=0.5,\n",
        "\tcolor='blue',\n",
        "\tfill=True,\n",
        "\tlabel='Entropy'\n",
        ");\n",
        "entropies.set(title='Entropy');\n",
        "entropies.set(xlabel='Bits');\n",
        "# entropies.set(xlim=(0));\n",
        "# entropies.xaxis.set_major_locator(ticker.MultipleLocator(2));\n",
        "# entropies.xaxis.set_major_formatter(ticker.ScalarFormatter());\n",
        "\n",
        "plt.show();\n",
        "\n",
        "\n",
        "#### DIFF(NLL, ENTROPY)\n",
        "diff_ic_ent = torch.sub(all_ics, all_entropies);\n",
        "# density_diff_ic_ent = sns.kdeplot(\n",
        "# \tdata=diff_ic_ent,\n",
        "# \tbw_method=1,\n",
        "# \tcolor='red',\n",
        "# \tfill=True,\n",
        "# \tlabel='diff'\n",
        "# \t# clip=(0, 100)\n",
        "# )\n",
        "# density_diff_ic_ent.set(\n",
        "# \ttitle='diff(IC, entropy)',\n",
        "# \txlabel='Bits',\n",
        "# \tylabel='Density'\n",
        "# )\n",
        "\n",
        "# abs(diff)\n",
        "abs_diff_ic_ent = torch.abs(diff_ic_ent);\n",
        "\n",
        "density_abs_diff_ic_ent = sns.kdeplot(\n",
        "\tdata=abs_diff_ic_ent,\n",
        "\tbw_method=1,\n",
        "\tcolor='orange',\n",
        "\tfill=True,\n",
        "\tlabel='abs. diff'\n",
        "\t# clip=(0, 100)\n",
        ");\n",
        "\n",
        "# sq(diff)\n",
        "sq_diff_ic_ent = torch.square(diff_ic_ent)\n",
        "density_sq_diff_ic_ent = sns.kdeplot(\n",
        "\tdata=sq_diff_ic_ent,\n",
        "\tbw_method=1,\n",
        "\tcolor='yellow',\n",
        "\tfill=True,\n",
        "\tlabel='sq. diff'\n",
        "\t# clip=(0, 100)\n",
        ");\n",
        "\n",
        "xmax = max(sq_diff_ic_ent)+1;\n",
        "density_sq_diff_ic_ent.set_title('Deviation of IC from expected IC');\n",
        "density_sq_diff_ic_ent.legend(loc=0);\n",
        "density_sq_diff_ic_ent.set(xlim=(0, xmax));\n",
        "density_sq_diff_ic_ent.set(xlabel='Bits');\n",
        "\n",
        "plt.show();\n",
        "\n",
        "print(f'Mean abs diff: { torch.mean(abs_diff_ic_ent) }');\n",
        "print(f'Median abs diff: { torch.median(abs_diff_ic_ent) }');\n",
        "print(f'Max abs diff: { torch.max(abs_diff_ic_ent) }');\n",
        "print(f'Min abs diff: { torch.min(abs_diff_ic_ent) }');"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "7c_GW-A-EaMz",
        "-aR_UNI5EaM0",
        "6YuGoaBKEaM3",
        "4eT24usadLqc",
        "aITyqsV2bcyU",
        "K00kF24GbkfW",
        "oCvlGaqKFD64",
        "ISJSn0x7FD64",
        "_JIhYq17UBiI",
        "Ams3120jFD63"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.8 ('venv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "a2bd9759b4dd9b4ffb5d0d7910e9887291e40cef96f156b9e33f4ee2c422f52d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
